{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "d0ab2cd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import load_model\n",
    "\n",
    "# Load the model from the .h5 file\n",
    "seq2seq_model1 = load_model('nmt.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "177e0376",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Asus\\AppData\\Local\\Temp\\ipykernel_21156\\3335428549.py:2: ParserWarning: Length of header or names does not match length of data. This leads to a loss of data with index_col=False.\n",
      "  lines= pd.read_table('fra.txt', names=['eng', 'fra'],index_col=False,nrows = 200000)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "lines= pd.read_table('fra.txt', names=['eng', 'fra'],index_col=False,nrows = 200000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "8d1ca63c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "lines.eng = lines.eng.apply(lambda x:x.lower())\n",
    "lines.fra = lines.fra.apply(lambda x:x.lower())\n",
    "punctuation = set(string.punctuation)\n",
    "lines.eng = lines.eng.apply(lambda x:''.join(char for char in x if char not in punctuation))\n",
    "lines.fra = lines.fra.apply(lambda x:''.join(char for char in x if char not in punctuation))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "c7562eeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "digits = set(string.digits)\n",
    "lines.eng = lines.eng.apply(lambda x:''.join(char for char in x if char not in digits))\n",
    "lines.fra = lines.fra.apply(lambda x:''.join(char for char in x if char not in digits))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "96a67cf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "# Remove extra spaces\n",
    "lines.eng=lines.eng.apply(lambda x: x.strip())\n",
    "lines.fra=lines.fra.apply(lambda x: x.strip())\n",
    "lines.eng=lines.eng.apply(lambda x: re.sub(\" +\", \" \", x))\n",
    "lines.fra=lines.fra.apply(lambda x: re.sub(\" +\", \" \", x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "ec2b42a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "lines.fra = lines.fra.apply(lambda x : '<SOS> '+ x + ' <EOS>')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "d130dff5",
   "metadata": {},
   "outputs": [],
   "source": [
    "english_words = set()\n",
    "for line in lines.eng:\n",
    "    for word in line.split():\n",
    "        if word not in english_words:\n",
    "            english_words.add(word)\n",
    "french_words = set()\n",
    "for line in lines.fra:\n",
    "    for word in line.split():\n",
    "        if word not in french_words:\n",
    "            french_words.add(word)            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "f45d5cb2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(13720, 28302)"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "total_english_words = len(english_words)\n",
    "total_french_words = len(french_words)\n",
    "total_english_words,total_french_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "683b12fa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "12"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "eng_len = []\n",
    "for line in lines.eng:\n",
    "    eng_len.append(len(line.split()))\n",
    "max_english_sen_len = np.max(eng_len)\n",
    "max_english_sen_len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "abc0f141",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "22"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fra_len = []\n",
    "for line in lines.fra:\n",
    "    fra_len.append(len(line.split()))\n",
    "max_french_sen_len = np.max(fra_len)\n",
    "max_french_sen_len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "53163743",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(13721, 28304)"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_words = sorted(list(english_words))\n",
    "target_words = sorted(list(french_words))\n",
    "num_encoder_tokens = len(english_words)+1\n",
    "num_decoder_tokens = len(french_words)+1\n",
    "num_decoder_tokens+=1 #zero padding\n",
    "num_encoder_tokens, num_decoder_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "d89ff3c7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((160000,), (40000,))"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "x, y = lines.eng, lines.fra\n",
    "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size = 0.2)\n",
    "x_train.shape, x_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "ada5581a",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_token_index = dict([(word, i+1) for i, word in enumerate(input_words)])\n",
    "target_token_index = dict([(word, i+1) for i, word in enumerate(target_words)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "78ce77a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "reverse_input_char_index = dict((i, word) for word, i in input_token_index.items())\n",
    "reverse_target_char_index = dict((i, word) for word, i in target_token_index.items())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "2378024b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open(\"input_token_index.pkl\", \"wb\") as f:\n",
    "    pickle.dump(input_token_index, f)\n",
    "with open(\"target_token_index.pkl\", \"wb\") as f:\n",
    "    pickle.dump(target_token_index, f)    \n",
    "with open(\"reverse_target_char_index.pkl\", \"wb\") as f:\n",
    "    pickle.dump(reverse_target_char_index, f)      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "b5b3eac4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[5981, 375, 1, 1434]\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "12662\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "24578\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "26640\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "10695\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "20474\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "12662\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "24578\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "26640\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "10695\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "20474\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "12662\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "24578\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "26640\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "10695\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "20474\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "12662\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "24578\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "26640\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "10695\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "20474\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "12662\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "24578\n",
      "Input: i am a boy\n",
      "Predicted Translation: je suis un gar√ßon \n"
     ]
    }
   ],
   "source": [
    "def translate(input_sentence):\n",
    "    # Preprocess input sentence\n",
    "    input_sentence = input_sentence.lower()\n",
    "    input_sentence = ''.join(char for char in input_sentence if char not in punctuation)\n",
    "    input_sentence = ''.join(char for char in input_sentence if char not in digits)\n",
    "    input_sentence = input_sentence.strip()\n",
    "    input_sentence = re.sub(\" +\", \" \", input_sentence)\n",
    "    \n",
    "    # Tokenize input sentence\n",
    "    input_words = input_sentence.split()\n",
    "    input_indices = [input_token_index.get(word, 0) for word in input_words]\n",
    "    print(input_indices)\n",
    "    # Create encoder input\n",
    "    encoder_input_data = np.zeros((1, max_english_sen_len), dtype='float32')\n",
    "    for t, index in enumerate(input_indices):\n",
    "        encoder_input_data[0, t] = index\n",
    "    \n",
    "    # Initial decoder input\n",
    "    decoder_input_data = np.zeros((1, 1), dtype='float32')\n",
    "    decoder_input_data[0, 0] = target_token_index['<SOS>']\n",
    "    \n",
    "    # Translate using the model\n",
    "    translation = ''\n",
    "    for _ in range(max_french_sen_len):\n",
    "        decoder_output = seq2seq_model1.predict([encoder_input_data, decoder_input_data])\n",
    "        predicted_token_index = np.argmax(decoder_output[0, -1, :])\n",
    "        print(predicted_token_index)\n",
    "        predicted_word = reverse_target_char_index.get(predicted_token_index, '<UNK>')\n",
    "        \n",
    "        if predicted_word == '<EOS>':\n",
    "            break\n",
    "        \n",
    "        translation += predicted_word + ' '\n",
    "        \n",
    "        # Update decoder input for next iteration\n",
    "        decoder_input_data = np.zeros((1, 1), dtype='float32')\n",
    "        decoder_input_data[0, 0] = predicted_token_index\n",
    "    \n",
    "    return translation.strip()\n",
    "\n",
    "input_sentence = \"i am a boy\"\n",
    "predicted_translation = translate(input_sentence)\n",
    "word_count = len(input_sentence.split())\n",
    "words = len(predicted_translation.split())\n",
    "prediction = \"\"\n",
    "i = 0\n",
    "for word in predicted_translation.split():\n",
    "    prediction+=word\n",
    "    prediction+=\" \"\n",
    "    i+=1\n",
    "    if i==word_count:\n",
    "        break\n",
    "\n",
    "print(\"Input:\", input_sentence)\n",
    "print(\"Predicted Translation:\", prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31fee71d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
